---
layout: post
title:  "Model Predictive Adversarial Imitation Learning for Planning from Observation"
date:   2025-01-31 22:21:59 +00:00
end_date:   Present
image: /images/mpail-fig1.png
categories: research
author: "Tyler Han"
authors: "<strong>Tyler Han</strong>, Yanda Bao, Bhaumik Mehta, Gabriel Guo, Anubhav Vishwakarma, Emily Kang, Sanghun Jung, Rosario Scalise, Jason Zhou, Bryan Xu, Byron Boots"
venue: Under Review
arxiv: https://arxiv.org/abs/2507.21533
other:
    title: code coming soon
    link: None
---
Human demonstration data is often ambiguous and incomplete, motivating imitation learning approaches that also exhibit reliable planning behavior. A common paradigm to perform planning-from-demonstration involves learning a reward function via Inverse Reinforcement Learning (IRL) then deploying this reward via Model Predictive Control (MPC). Towards unifying these methods, we derive a replacement of the policy in IRL with a planning-based agent. With connections to Adversarial Imitation Learning, this formulation enables end-to-end interactive learning of planners from observation-only demonstrations.
In addition to benefits in interpretability, complexity, and safety, 
we study and observe significant improvements on sample efficiency, out-of-distribution generalization, and robustness.
The study includes evaluations in both simulated control benchmarks and
real-world navigation experiments using few-to-single observation-only demonstrations.